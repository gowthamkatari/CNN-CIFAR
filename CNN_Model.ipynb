{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Dataset consists of 60000 32x32 colour images.\n",
    "Image belong to 10 classes, with 6000 images per classs.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependecies\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle('data_batches/%s'%direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'filenames', b'data', b'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of single image before rreshaping\n",
    "len(data_batch1[b\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get image data from first batch\n",
    "X = data_batch1[b\"data\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape and transpose to display image using imshow\n",
    "X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11310e9e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1130f0080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display sample Image\n",
    "plt.imshow(X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENcode lablels to One-hot format\n",
    "def one_hot_encode(vec, vals=10):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to prepare data to feeding into the model\n",
    "class CifarHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        # List of all the data batches for training and testing\n",
    "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
    "        self.test_batch = [test_batch]\n",
    "        \n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        \n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Training Images and Labels\")\n",
    "        \n",
    "        # vertically stack training images\n",
    "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        \n",
    "        # Reshape and normalizes the training images \n",
    "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        \n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
    "        \n",
    "        print(\"Setting Up Test Images and Labels\")\n",
    "        \n",
    "        # vertically stack test images\n",
    "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        \n",
    "        # Reshape and normalizes the test images \n",
    "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
    "\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Training Images and Labels\n",
      "Setting Up Test Images and Labels\n"
     ]
    }
   ],
   "source": [
    "ch = CifarHelper()\n",
    "ch.set_up_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Placeholdes to feed data\n",
    "x = tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper function\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 1 convolution layer and pooling layers\n",
    "convo_1 = convolutional_layer(x,shape=[4,4,3,32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2 convolution layer and pooling layers\n",
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[4,4,32,64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create flatten layer by reshaping the pooling layer into [-1,8*8*64] or [-1,4096]\n",
    "convo_2_flat = tf.reshape(convo_2_pooling,[-1,8*8*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create fully connected layer with 1024 neurons by passing flattened layer\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout layer\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted values\n",
    "y_pred = normal_full_layer(full_one_dropout,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function (cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer ( Adam optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Accuracy is:\n",
      "0.0992\n",
      "\n",
      "\n",
      "Currently on step 100\n",
      "Accuracy is:\n",
      "0.4165\n",
      "\n",
      "\n",
      "Currently on step 200\n",
      "Accuracy is:\n",
      "0.4344\n",
      "\n",
      "\n",
      "Currently on step 300\n",
      "Accuracy is:\n",
      "0.4955\n",
      "\n",
      "\n",
      "Currently on step 400\n",
      "Accuracy is:\n",
      "0.516\n",
      "\n",
      "\n",
      "Currently on step 500\n",
      "Accuracy is:\n",
      "0.5515\n",
      "\n",
      "\n",
      "Currently on step 600\n",
      "Accuracy is:\n",
      "0.561\n",
      "\n",
      "\n",
      "Currently on step 700\n",
      "Accuracy is:\n",
      "0.5888\n",
      "\n",
      "\n",
      "Currently on step 800\n",
      "Accuracy is:\n",
      "0.601\n",
      "\n",
      "\n",
      "Currently on step 900\n",
      "Accuracy is:\n",
      "0.5986\n",
      "\n",
      "\n",
      "Currently on step 1000\n",
      "Accuracy is:\n",
      "0.6252\n",
      "\n",
      "\n",
      "Currently on step 1100\n",
      "Accuracy is:\n",
      "0.6223\n",
      "\n",
      "\n",
      "Currently on step 1200\n",
      "Accuracy is:\n",
      "0.6158\n",
      "\n",
      "\n",
      "Currently on step 1300\n",
      "Accuracy is:\n",
      "0.6399\n",
      "\n",
      "\n",
      "Currently on step 1400\n",
      "Accuracy is:\n",
      "0.631\n",
      "\n",
      "\n",
      "Currently on step 1500\n",
      "Accuracy is:\n",
      "0.6499\n",
      "\n",
      "\n",
      "Currently on step 1600\n",
      "Accuracy is:\n",
      "0.6597\n",
      "\n",
      "\n",
      "Currently on step 1700\n",
      "Accuracy is:\n",
      "0.661\n",
      "\n",
      "\n",
      "Currently on step 1800\n",
      "Accuracy is:\n",
      "0.6714\n",
      "\n",
      "\n",
      "Currently on step 1900\n",
      "Accuracy is:\n",
      "0.6701\n",
      "\n",
      "\n",
      "Currently on step 2000\n",
      "Accuracy is:\n",
      "0.6798\n",
      "\n",
      "\n",
      "Currently on step 2100\n",
      "Accuracy is:\n",
      "0.6785\n",
      "\n",
      "\n",
      "Currently on step 2200\n",
      "Accuracy is:\n",
      "0.6736\n",
      "\n",
      "\n",
      "Currently on step 2300\n",
      "Accuracy is:\n",
      "0.6766\n",
      "\n",
      "\n",
      "Currently on step 2400\n",
      "Accuracy is:\n",
      "0.6876\n",
      "\n",
      "\n",
      "Currently on step 2500\n",
      "Accuracy is:\n",
      "0.6931\n",
      "\n",
      "\n",
      "Currently on step 2600\n",
      "Accuracy is:\n",
      "0.6947\n",
      "\n",
      "\n",
      "Currently on step 2700\n",
      "Accuracy is:\n",
      "0.6873\n",
      "\n",
      "\n",
      "Currently on step 2800\n",
      "Accuracy is:\n",
      "0.68\n",
      "\n",
      "\n",
      "Currently on step 2900\n",
      "Accuracy is:\n",
      "0.6966\n",
      "\n",
      "\n",
      "Currently on step 3000\n",
      "Accuracy is:\n",
      "0.6968\n",
      "\n",
      "\n",
      "Currently on step 3100\n",
      "Accuracy is:\n",
      "0.6819\n",
      "\n",
      "\n",
      "Currently on step 3200\n",
      "Accuracy is:\n",
      "0.6771\n",
      "\n",
      "\n",
      "Currently on step 3300\n",
      "Accuracy is:\n",
      "0.6927\n",
      "\n",
      "\n",
      "Currently on step 3400\n",
      "Accuracy is:\n",
      "0.6959\n",
      "\n",
      "\n",
      "Currently on step 3500\n",
      "Accuracy is:\n",
      "0.7049\n",
      "\n",
      "\n",
      "Currently on step 3600\n",
      "Accuracy is:\n",
      "0.6994\n",
      "\n",
      "\n",
      "Currently on step 3700\n",
      "Accuracy is:\n",
      "0.6876\n",
      "\n",
      "\n",
      "Currently on step 3800\n",
      "Accuracy is:\n",
      "0.6995\n",
      "\n",
      "\n",
      "Currently on step 3900\n",
      "Accuracy is:\n",
      "0.6958\n",
      "\n",
      "\n",
      "Currently on step 4000\n",
      "Accuracy is:\n",
      "0.6903\n",
      "\n",
      "\n",
      "Currently on step 4100\n",
      "Accuracy is:\n",
      "0.7017\n",
      "\n",
      "\n",
      "Currently on step 4200\n",
      "Accuracy is:\n",
      "0.6923\n",
      "\n",
      "\n",
      "Currently on step 4300\n",
      "Accuracy is:\n",
      "0.6906\n",
      "\n",
      "\n",
      "Currently on step 4400\n",
      "Accuracy is:\n",
      "0.7119\n",
      "\n",
      "\n",
      "Currently on step 4500\n",
      "Accuracy is:\n",
      "0.7057\n",
      "\n",
      "\n",
      "Currently on step 4600\n",
      "Accuracy is:\n",
      "0.7096\n",
      "\n",
      "\n",
      "Currently on step 4700\n",
      "Accuracy is:\n",
      "0.7032\n",
      "\n",
      "\n",
      "Currently on step 4800\n",
      "Accuracy is:\n",
      "0.7029\n",
      "\n",
      "\n",
      "Currently on step 4900\n",
      "Accuracy is:\n",
      "0.701\n",
      "\n",
      "\n",
      "Currently on step 5000\n",
      "Accuracy is:\n",
      "0.7041\n",
      "\n",
      "\n",
      "Currently on step 5100\n",
      "Accuracy is:\n",
      "0.7094\n",
      "\n",
      "\n",
      "Currently on step 5200\n",
      "Accuracy is:\n",
      "0.6971\n",
      "\n",
      "\n",
      "Currently on step 5300\n",
      "Accuracy is:\n",
      "0.7082\n",
      "\n",
      "\n",
      "Currently on step 5400\n",
      "Accuracy is:\n",
      "0.6994\n",
      "\n",
      "\n",
      "Currently on step 5500\n",
      "Accuracy is:\n",
      "0.7025\n",
      "\n",
      "\n",
      "Currently on step 5600\n",
      "Accuracy is:\n",
      "0.7085\n",
      "\n",
      "\n",
      "Currently on step 5700\n",
      "Accuracy is:\n",
      "0.7032\n",
      "\n",
      "\n",
      "Currently on step 5800\n",
      "Accuracy is:\n",
      "0.7046\n",
      "\n",
      "\n",
      "Currently on step 5900\n",
      "Accuracy is:\n",
      "0.7022\n",
      "\n",
      "\n",
      "Currently on step 6000\n",
      "Accuracy is:\n",
      "0.7042\n",
      "\n",
      "\n",
      "Currently on step 6100\n",
      "Accuracy is:\n",
      "0.7123\n",
      "\n",
      "\n",
      "Currently on step 6200\n",
      "Accuracy is:\n",
      "0.7074\n",
      "\n",
      "\n",
      "Currently on step 6300\n",
      "Accuracy is:\n",
      "0.7111\n",
      "\n",
      "\n",
      "Currently on step 6400\n",
      "Accuracy is:\n",
      "0.7021\n",
      "\n",
      "\n",
      "Currently on step 6500\n",
      "Accuracy is:\n",
      "0.698\n",
      "\n",
      "\n",
      "Currently on step 6600\n",
      "Accuracy is:\n",
      "0.7116\n",
      "\n",
      "\n",
      "Currently on step 6700\n",
      "Accuracy is:\n",
      "0.7025\n",
      "\n",
      "\n",
      "Currently on step 6800\n",
      "Accuracy is:\n",
      "0.7171\n",
      "\n",
      "\n",
      "Currently on step 6900\n",
      "Accuracy is:\n",
      "0.7008\n",
      "\n",
      "\n",
      "Currently on step 7000\n",
      "Accuracy is:\n",
      "0.714\n",
      "\n",
      "\n",
      "Currently on step 7100\n",
      "Accuracy is:\n",
      "0.7106\n",
      "\n",
      "\n",
      "Currently on step 7200\n",
      "Accuracy is:\n",
      "0.716\n",
      "\n",
      "\n",
      "Currently on step 7300\n",
      "Accuracy is:\n",
      "0.7122\n",
      "\n",
      "\n",
      "Currently on step 7400\n",
      "Accuracy is:\n",
      "0.7017\n",
      "\n",
      "\n",
      "Currently on step 7500\n",
      "Accuracy is:\n",
      "0.7176\n",
      "\n",
      "\n",
      "Currently on step 7600\n",
      "Accuracy is:\n",
      "0.7045\n",
      "\n",
      "\n",
      "Currently on step 7700\n",
      "Accuracy is:\n",
      "0.7092\n",
      "\n",
      "\n",
      "Currently on step 7800\n",
      "Accuracy is:\n",
      "0.7085\n",
      "\n",
      "\n",
      "Currently on step 7900\n",
      "Accuracy is:\n",
      "0.6974\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cc08c1d5e7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# PRINT OUT A MESSAGE EVERY 100 STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(1000):\n",
    "        batch = ch.next_batch(100)\n",
    "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
