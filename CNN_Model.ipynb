{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Dataset consists of 60000 32x32 colour images.\n",
    "Image belong to 10 classes, with 6000 images per classs.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependecies\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle('data_batches/%s'%direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'data', b'batch_label', b'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of single image before rreshaping\n",
    "len(data_batch1[b\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image data from first batch\n",
    "X = data_batch1[b\"data\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape and transpose to display image using imshow\n",
    "X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1151d59b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlHk/EIkyv8Dbs3Lfjigaw4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display sample Image\n",
    "plt.imshow(X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENcode lablels to One-hot format\n",
    "def one_hot_encode(vec, vals=10):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to prepare data to feeding into the model\n",
    "class CifarHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        # List of all the data batches for training and testing\n",
    "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
    "        self.test_batch = [test_batch]\n",
    "        \n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        \n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Training Images and Labels\")\n",
    "        \n",
    "        # vertically stack training images\n",
    "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        \n",
    "        # Reshape and normalizes the training images \n",
    "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        \n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
    "        \n",
    "        print(\"Setting Up Test Images and Labels\")\n",
    "        \n",
    "        # vertically stack test images\n",
    "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        \n",
    "        # Reshape and normalizes the test images \n",
    "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
    "\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Training Images and Labels\n",
      "Setting Up Test Images and Labels\n"
     ]
    }
   ],
   "source": [
    "ch = CifarHelper()\n",
    "ch.set_up_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Placeholdes to feed data\n",
    "x = tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper function\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 1 convolution layer and pooling layers\n",
    "convo_1 = convolutional_layer(x,shape=[4,4,3,32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2 convolution layer and pooling layers\n",
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[4,4,32,64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create flatten layer by reshaping the pooling layer into [-1,8*8*64] or [-1,4096]\n",
    "convo_2_flat = tf.reshape(convo_2_pooling,[-1,8*8*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create fully connected layer with 1024 neurons by passing flattened layer\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout layer\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted values\n",
    "y_pred = normal_full_layer(full_one_dropout,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function (cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer ( Adam optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Accuracy is:\n",
      "0.1187\n",
      "\n",
      "\n",
      "Currently on step 100\n",
      "Accuracy is:\n",
      "0.4239\n",
      "\n",
      "\n",
      "Currently on step 200\n",
      "Accuracy is:\n",
      "0.47\n",
      "\n",
      "\n",
      "Currently on step 300\n",
      "Accuracy is:\n",
      "0.5044\n",
      "\n",
      "\n",
      "Currently on step 400\n",
      "Accuracy is:\n",
      "0.5165\n",
      "\n",
      "\n",
      "Currently on step 500\n",
      "Accuracy is:\n",
      "0.5571\n",
      "\n",
      "\n",
      "Currently on step 600\n",
      "Accuracy is:\n",
      "0.5756\n",
      "\n",
      "\n",
      "Currently on step 700\n",
      "Accuracy is:\n",
      "0.576\n",
      "\n",
      "\n",
      "Currently on step 800\n",
      "Accuracy is:\n",
      "0.5905\n",
      "\n",
      "\n",
      "Currently on step 900\n",
      "Accuracy is:\n",
      "0.6051\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(1000):\n",
    "        batch = ch.next_batch(100)\n",
    "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
